{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AdaBoost Decision Tree - Regression (Scratch)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AcBMz4mDPvTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "txc-dnUTvyY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, leaf=False, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.leaf = leaf\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "x8LBOceBPs5t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, criterion='mse', max_depth=None, min_samples_split=2):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or len(np.unique(y)) == 1 or n_samples < self.min_samples_split:\n",
        "            return self._create_leaf_node(y)\n",
        "\n",
        "        best_feature, best_threshold = self._find_best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            return self._create_leaf_node(y)\n",
        "\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = X[:, best_feature] >= best_threshold\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        best_feature, best_threshold, best_gain = None, None, -np.inf\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature] < threshold\n",
        "                right_indices = X[:, feature] >= threshold\n",
        "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = self._calculate_gain(y, y[left_indices], y[right_indices])\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _calculate_gain(self, y, left_y, right_y):\n",
        "        parent_loss = np.var(y)\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(left_y), len(right_y)\n",
        "\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0\n",
        "\n",
        "        child_loss = (n_left / n * np.var(left_y)) + (n_right / n * np.var(right_y))\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _create_leaf_node(self, y):\n",
        "        leaf_value = np.mean(y)  # Average value for regression\n",
        "        return Node(leaf=True, value=leaf_value)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(sample, self.root) for sample in X])\n",
        "\n",
        "    def _predict(self, sample, tree):\n",
        "        if tree.leaf:\n",
        "            return tree.value\n",
        "        if sample[tree.feature] < tree.threshold:\n",
        "            return self._predict(sample, tree.left)\n",
        "        else:\n",
        "            return self._predict(sample, tree.right)"
      ],
      "metadata": {
        "id": "XRIfWw_gTkX_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "\n",
        "    def fit(self, X, y, model):\n",
        "        n_samples = len(y)\n",
        "        self.models = []\n",
        "        self.alphas = []\n",
        "        w = np.ones(n_samples) / n_samples  # Initialize weights\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            model.fit(X, y)  # Fit the model\n",
        "            y_pred = model.predict(X)\n",
        "\n",
        "            # Calculate the error\n",
        "            error = np.abs(y - y_pred)\n",
        "            error_rate = np.dot(w, error) / np.sum(w)  # Weighted error rate\n",
        "\n",
        "            if error_rate <= 0:\n",
        "                print(\"Perfect prediction encountered. Stopping training.\")\n",
        "                break\n",
        "\n",
        "            alpha = 0.5 * np.log((1 - error_rate) / (error_rate + 1e-10))  # Avoid division by zero\n",
        "            self.models.append(model)\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "            # Update weights\n",
        "            w *= np.exp(-alpha * (y - y_pred))\n",
        "            w /= np.sum(w)  # Normalize weights\n",
        "\n",
        "            if np.all(error < 1e-10):  # Check if all errors are very small\n",
        "                print(\"All errors are very small. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        print(f\"Number of trees used: {len(self.models)}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for model, alpha in zip(self.models, self.alphas):\n",
        "            y_pred += alpha * model.predict(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "sAlbZWaITcDZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "oyCrZ9XITdHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple dataset\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Features\n",
        "y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Target values"
      ],
      "metadata": {
        "id": "hI1HgMy_TG-R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a5OY3XrTHA7",
        "outputId": "6e59ad87-3306-4a2c-84fe-003dfd4a3be7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 1), (2, 1), (8,), (2,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new decision tree regressor\n",
        "model = DecisionTreeRegressor(criterion='mse', max_depth=5, min_samples_split=3)"
      ],
      "metadata": {
        "id": "uBqzhQzaTHDz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the AdaBoost regressor\n",
        "adaboost_model = AdaBoost(n_estimators=50)\n",
        "adaboost_model.fit(X_train, y_train, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Ld4PK9THMx",
        "outputId": "f3a59a9d-bc02-4b47-92d2-30ff578921de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trees used: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeFaI5VvTQu4",
        "outputId": "50ed7447-9f57-455a-93e9-cc130507eeb1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56.89362619  7.58581683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"R^2 Score: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKHc_P9cTQxu",
        "outputId": "d2209cc4-90dd-4edc-c70f-d60983a5c18f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1162.5003896661963\n",
            "Mean Absolute Error: 26.739721508241985\n",
            "R^2 Score: -93.89799099315888\n"
          ]
        }
      ]
    }
  ]
}