{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AdaBoost Random Forest - Regression (Scratch)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcBMz4mDPvTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "0n4DS8KO415I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index, threshold, left, right):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right"
      ],
      "metadata": {
        "id": "R-9-gsL9668Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafNodeRegression:\n",
        "    def __init__(self, y):\n",
        "        self.value = np.mean(y)\n",
        "\n",
        "    def predicted_value(self):\n",
        "        return self.value"
      ],
      "metadata": {
        "id": "LBJcxISF66_P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeRegression:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"mse\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # Check stopping criteria\n",
        "        if depth >= self.max_depth or num_samples < self.min_samples_split:\n",
        "            return LeafNodeRegression(y)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature_index, best_threshold, indices_left, indices_right = self._best_split(X, y, num_features)\n",
        "        if best_feature_index is None:\n",
        "            return LeafNodeRegression(y)\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_subtree = self._build_tree(X[indices_left], y[indices_left], depth + 1)\n",
        "        right_subtree = self._build_tree(X[indices_right], y[indices_right], depth + 1)\n",
        "\n",
        "        return Node(best_feature_index, best_threshold, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, num_features):\n",
        "        best_gain = -1\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "        best_indices_left = None\n",
        "        best_indices_right = None\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                indices_left = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                indices_right = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(indices_left) > 0 and len(indices_right) > 0:\n",
        "                    gain = self._information_gain(y, indices_left, indices_right)\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_feature_index = feature_index\n",
        "                        best_threshold = threshold\n",
        "                        best_indices_left = indices_left\n",
        "                        best_indices_right = indices_right\n",
        "\n",
        "        return best_feature_index, best_threshold, best_indices_left, best_indices_right\n",
        "\n",
        "    def _information_gain(self, y, left_indices, right_indices):\n",
        "        impurity_before = self._impurity(y)\n",
        "        impurity_left = self._impurity(y[left_indices])\n",
        "        impurity_right = self._impurity(y[right_indices])\n",
        "\n",
        "        weighted_impurity = (len(left_indices) / len(y)) * impurity_left + (len(right_indices) / len(y)) * impurity_right\n",
        "        return impurity_before - weighted_impurity\n",
        "\n",
        "    def _impurity(self, y):\n",
        "        if self.criterion == \"mse\":\n",
        "            return np.mean((y - np.mean(y)) ** 2)\n",
        "        elif self.criterion == \"mae\":\n",
        "            return np.mean(np.abs(y - np.mean(y)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, LeafNodeRegression):\n",
        "            return tree.predicted_value()\n",
        "        else:\n",
        "            if x[tree.feature_index] <= tree.threshold:\n",
        "                return self._traverse_tree(x, tree.left)\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree.right)"
      ],
      "metadata": {
        "id": "f85hsu6U64gO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForestRegression:\n",
        "    def __init__(self, n_trees=5, max_depth=10, min_samples_split=2, criterion=\"mse\"):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTreeRegression(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion)\n",
        "            # Bootstrap sampling\n",
        "            sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample = X[sample_indices]\n",
        "            y_sample = y[sample_indices]\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Average the predictions from all trees\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_predictions, axis=0)"
      ],
      "metadata": {
        "id": "rFK69akw60RR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=5, learning_rate=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "        self.alphas = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = len(y)\n",
        "        # Initialize weights\n",
        "        weights = np.ones(n_samples) / n_samples\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            model = DecisionTreeRegression(max_depth=1, min_samples_split=1)  # Weak learner (stump)\n",
        "            model.fit(X, y)\n",
        "            y_pred = model.predict(X)\n",
        "\n",
        "            # Compute error\n",
        "            error = np.sum(weights * (y_pred != y)) / np.sum(weights)\n",
        "            alpha = self.learning_rate * 0.5 * np.log((1 - error) / (error + 1e-10))  # Adding small value to avoid division by zero\n",
        "\n",
        "            # Update weights\n",
        "            weights *= np.exp(-alpha * y * y_pred)\n",
        "            weights /= np.sum(weights)  # Normalize weights\n",
        "\n",
        "            self.models.append(model)\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the weighted sum of the predictions from all models\n",
        "        final_prediction = sum(alpha * model.predict(X) for alpha, model in zip(self.alphas, self.models))\n",
        "        return np.sign(final_prediction)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        mse = np.mean((y - predictions) ** 2)\n",
        "        return mse"
      ],
      "metadata": {
        "id": "fOCgfPLA5up3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "IzlxsAFM59D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load the diabetes dataset for regression\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert targets to 1 for regression (this is just for the sake of AdaBoost)\n",
        "y_binary = np.where(y > np.mean(y), 1, -1)  # Converting to binary for AdaBoost compatibility"
      ],
      "metadata": {
        "id": "LqwGvcyE56rw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CDyoOQWR56uq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train multiple Random Forest regressors\n",
        "num_forests = 3\n",
        "forests = []\n",
        "\n",
        "for i in range(num_forests):\n",
        "    print(f\"Training Forest {i + 1} with {5} trees...\")\n",
        "    forest_regressor = RandomForestRegression(n_trees=5, max_depth=10, min_samples_split=2, criterion=\"mse\")\n",
        "    forest_regressor.fit(X_train, y_train)\n",
        "    forests.append(forest_regressor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO7fZ2ev56xQ",
        "outputId": "5aa456a2-5a1e-47cf-b3e8-63e515d54901"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Forest 1 with 5 trees...\n",
            "Training Forest 2 with 5 trees...\n",
            "Training Forest 3 with 5 trees...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AdaBoost using the Random Forests as base estimators\n",
        "for i in range(num_forests):\n",
        "    print(f\"\\nTraining AdaBoost with Forest {i + 1}...\")\n",
        "    adaboost = AdaBoost(n_estimators=5, learning_rate=1)\n",
        "    adaboost.fit(X_train, y_train)\n",
        "    y_pred = adaboost.predict(X_test)\n",
        "    mse = adaboost.evaluate(X_test, y_test)\n",
        "    print(f\"AdaBoost with Forest {i + 1} MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro8AAn_G56zl",
        "outputId": "7d35555a-3976-48a3-bc99-f1f8ee87b553"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training AdaBoost with Forest 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cbbbccb229d9>:20: RuntimeWarning: divide by zero encountered in log\n",
            "  alpha = self.learning_rate * 0.5 * np.log((1 - error) / (error + 1e-10))  # Adding small value to avoid division by zero\n",
            "<ipython-input-6-cbbbccb229d9>:24: RuntimeWarning: invalid value encountered in divide\n",
            "  weights /= np.sum(weights)  # Normalize weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost with Forest 1 MSE: nan\n",
            "\n",
            "Training AdaBoost with Forest 2...\n",
            "AdaBoost with Forest 2 MSE: nan\n",
            "\n",
            "Training AdaBoost with Forest 3...\n",
            "AdaBoost with Forest 3 MSE: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the number of forests and trees\n",
        "print(f\"\\nTotal number of forests used: {num_forests}\")\n",
        "for i in range(num_forests):\n",
        "    print(f\"Forest {i + 1} contains {5} trees.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqYT-zpL562T",
        "outputId": "495e32b2-75b0-4050-b7de-77e8e3f379a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of forests used: 3\n",
            "Forest 1 contains 5 trees.\n",
            "Forest 2 contains 5 trees.\n",
            "Forest 3 contains 5 trees.\n"
          ]
        }
      ]
    }
  ]
}