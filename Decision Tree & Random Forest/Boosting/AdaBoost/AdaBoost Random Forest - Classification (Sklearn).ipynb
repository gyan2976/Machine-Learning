{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AdaBoost Random Forest - Classification (Sklearn)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcBMz4mDPvTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Using Sklearn**"
      ],
      "metadata": {
        "id": "g0P_a9fR2Res"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset for demonstration\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=3, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of Random Forest classifiers to create\n",
        "num_forests = 3\n",
        "# Create multiple Random Forest classifiers\n",
        "random_forests = [(f'rf_{i}', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42 + i)) for i in range(num_forests)]\n",
        "\n",
        "# Initialize AdaBoost\n",
        "ada_boost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Create a VotingClassifier to combine them\n",
        "ensemble_model = VotingClassifier(estimators=random_forests + [('ada', ada_boost)], voting='hard')\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "print(y_pred_ensemble[:5])\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"Ensemble Model Accuracy: {accuracy_ensemble * 100:.2f}%\")\n",
        "\n",
        "# Retrieve information about the Random Forest models\n",
        "total_forests = len(random_forests)  # Total number of Random Forests used\n",
        "trees_per_forest = random_forests[0][1].n_estimators  # Number of trees in each Random Forest\n",
        "\n",
        "print(f\"Total Random Forests used: {total_forests}\")\n",
        "print(f\"Each Random Forest consists of {trees_per_forest} trees.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xD7qgoD2NPg",
        "outputId": "c65ed779-8af7-4e97-cd3c-23d1f696ce67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 0 0 1]\n",
            "Ensemble Model Accuracy: 74.00%\n",
            "Total Random Forests used: 3\n",
            "Each Random Forest consists of 100 trees.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Using AdaBoost Class**"
      ],
      "metadata": {
        "id": "hRTnQTxV2dsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoostForest:\n",
        "    def __init__(self, n_estimators=5, n_trees=10, max_depth=3, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators  # Number of boosting iterations\n",
        "        self.n_trees = n_trees  # Number of trees in each random forest\n",
        "        self.max_depth = max_depth  # Max depth of each tree\n",
        "        self.min_samples_split = min_samples_split  # Min samples to split a node\n",
        "        self.forests = []  # List to store Random Forest models\n",
        "        self.tree_weights = []  # Weights for each forest\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        sample_weights = np.full(n_samples, (1 / n_samples))  # Initial sample weights\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Create a Random Forest model for this iteration\n",
        "            forest = RandomForestClassifier(n_estimators=self.n_trees,\n",
        "                                             max_depth=self.max_depth,\n",
        "                                             min_samples_split=self.min_samples_split,\n",
        "                                             random_state=42)\n",
        "            forest.fit(X, y, sample_weight=sample_weights)  # Fit forest with sample weights\n",
        "\n",
        "            predictions = forest.predict(X)\n",
        "            error = np.sum(sample_weights * (predictions != y)) / np.sum(sample_weights)\n",
        "\n",
        "            # Calculate weight for the forest\n",
        "            tree_weight = 0.5 * np.log((1 - error) / (error + 1e-10))  # Avoid division by zero\n",
        "            self.forests.append(forest)\n",
        "            self.tree_weights.append(tree_weight)\n",
        "\n",
        "            # Update sample weights\n",
        "            sample_weights *= np.exp(-tree_weight * y * (2 * (predictions == y) - 1))\n",
        "            sample_weights /= np.sum(sample_weights)  # Normalize weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        weighted_predictions = np.zeros(X.shape[0])\n",
        "        for forest, tree_weight in zip(self.forests, self.tree_weights):\n",
        "            predictions = forest.predict(X)\n",
        "            weighted_predictions += tree_weight * predictions  # Accumulate weighted predictions\n",
        "\n",
        "        return np.sign(weighted_predictions).astype(int)  # Return class predictions\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            \"total_forests\": len(self.forests),\n",
        "            \"trees_per_forest\": self.n_trees\n",
        "        }\n",
        "\n",
        "# Generate a synthetic dataset for demonstration\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels from {0, 1} to {-1, 1} for compatibility with the AdaBoostForest implementation\n",
        "y_train = np.where(y_train == 0, -1, 1)\n",
        "y_test = np.where(y_test == 0, -1, 1)\n",
        "\n",
        "# Initialize and fit the AdaBoostForest model\n",
        "ada_boost_forest = AdaBoostForest(n_estimators=5, n_trees=10, max_depth=3)\n",
        "ada_boost_forest.fit(X_train, y_train)\n",
        "\n",
        "# Print information about the model\n",
        "info = ada_boost_forest.get_info()\n",
        "print(f\"Total Random Forests used: {info['total_forests']}\")\n",
        "print(f\"Each Random Forest consists of {info['trees_per_forest']} trees.\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada_boost_forest.predict(X_test)\n",
        "print(y_pred[:5])\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoostForest Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHM3yC7H2OBg",
        "outputId": "2892a3c9-fe86-43b3-c203-82205234ee6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Random Forests used: 5\n",
            "Each Random Forest consists of 10 trees.\n",
            "[-1 -1 -1 -1 -1]\n",
            "AdaBoostForest Accuracy: 59.50%\n"
          ]
        }
      ]
    }
  ]
}