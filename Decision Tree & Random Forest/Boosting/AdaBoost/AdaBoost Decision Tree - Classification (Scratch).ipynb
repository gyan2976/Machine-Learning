{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AdaBoost Decision Tree - Classification (Scratch)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcBMz4mDPvTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "txc-dnUTvyY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature        # Feature index to split on\n",
        "        self.threshold = threshold    # Threshold value for the split\n",
        "        self.left = left              # Left subtree\n",
        "        self.right = right            # Right subtree\n",
        "        self.value = value            # Class label for leaf nodes"
      ],
      "metadata": {
        "id": "saV670xbGzO9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        unique_classes, counts = np.unique(y, return_counts=True)\n",
        "        most_common_class = unique_classes[np.argmax(counts)]\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or len(unique_classes) == 1 or n_samples < self.min_samples_split:\n",
        "            return Node(value=most_common_class)\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            return Node(value=most_common_class)\n",
        "\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = X[:, best_feature] >= best_threshold\n",
        "\n",
        "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_tree, right=right_tree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        best_gain = -np.inf\n",
        "\n",
        "        for feature in range(X.shape[1]):\n",
        "            thresholds, classes = zip(*sorted(zip(X[:, feature], y)))\n",
        "            num_left = np.zeros(len(np.unique(y)), dtype=int)\n",
        "            num_right = np.bincount(classes)\n",
        "\n",
        "            for i in range(1, len(y)):\n",
        "                c = classes[i - 1]\n",
        "                num_left[c] += 1\n",
        "                num_right[c] -= 1\n",
        "\n",
        "                if thresholds[i] == thresholds[i - 1]:\n",
        "                    continue\n",
        "\n",
        "                gain = self._information_gain(y, num_left, num_right)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = (thresholds[i] + thresholds[i - 1]) / 2\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _information_gain(self, y, num_left, num_right):\n",
        "        p_left = num_left.sum() / y.size\n",
        "        p_right = num_right.sum() / y.size\n",
        "\n",
        "        if p_left == 0 or p_right == 0:\n",
        "            return 0\n",
        "\n",
        "        return self._gini_impurity(y) - (p_left * self._gini_impurity(num_left) + p_right * self._gini_impurity(num_right))\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        return 1 - sum((counts / counts.sum()) ** 2)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_sample(sample, self.root) for sample in X])\n",
        "\n",
        "    def _predict_sample(self, sample, node):\n",
        "        if node.value is not None:  # Leaf node\n",
        "            return node.value\n",
        "\n",
        "        if sample[node.feature] < node.threshold:\n",
        "            return self._predict_sample(sample, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(sample, node.right)"
      ],
      "metadata": {
        "id": "KYadlPpVGzSC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.models = []  # List to hold weak learners\n",
        "        self.alphas = []  # List to hold the weights of the weak learners\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Initialize weights uniformly\n",
        "        w = np.ones(n_samples) / n_samples\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Train a weak learner\n",
        "            model = DecisionTree(criterion='gini', max_depth=1, min_samples_split=2)\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X)\n",
        "\n",
        "            # Calculate error\n",
        "            error = np.sum(w * (y_pred != y)) / np.sum(w)\n",
        "\n",
        "            # Calculate alpha (weight of the weak learner)\n",
        "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
        "\n",
        "            # Update sample weights\n",
        "            w *= np.exp(-alpha * (y_pred == y) * 2 - 1)  # Adjusted for binary labels\n",
        "            w /= np.sum(w)  # Normalize weights\n",
        "\n",
        "            # Store the model and its alpha\n",
        "            self.models.append(model)\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Initialize predictions to 0\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "\n",
        "        # Aggregate predictions from each weak learner\n",
        "        for alpha, model in zip(self.alphas, self.models):\n",
        "            y_pred += alpha * model.predict(X)\n",
        "\n",
        "        # Return the sign of predictions (0 or 1)\n",
        "        return (y_pred > 0).astype(int)\n",
        "\n",
        "    def get_n_estimators(self):\n",
        "        return len(self.models)"
      ],
      "metadata": {
        "id": "NMkTglOmGzgy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "RWRlaSF7H01Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "GaF2BLpDGV2B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to binary classification for AdaBoost\n",
        "y = np.where(y == 2, 1, 0)  # Convert to 0 and 1 for AdaBoost"
      ],
      "metadata": {
        "id": "Ov_dIVnoHDmA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdO477XDHDow",
        "outputId": "5b678e53-6540-4917-bd36-d716202b40c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120, 4), (30, 4), (120,), (30,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AdaBoost\n",
        "adaboost = AdaBoost(n_estimators=50)\n",
        "adaboost.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "rkDKjg5WHFlH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of decision trees used\n",
        "num_trees = adaboost.get_n_estimators()\n",
        "print(f\"Number of Decision Trees Used: {num_trees}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHKwyIfqHd6r",
        "outputId": "d1de357f-5a1a-4abc-ba7a-9fdc354dd84e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Decision Trees Used: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions = adaboost.predict(X_test)\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqfpG6A7HFoU",
        "outputId": "210d29e0-5815-4cb5-dd7b-a5c98350b08f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"AdaBoost Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXS4zyvAHFq3",
        "outputId": "7d68363e-eee1-42b3-a6bb-ab96e75f15ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NdliNRKHKNn",
        "outputId": "d76222cf-e4f1-4067-c5c2-6bd38df0e32d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[19  0]\n",
            " [11  0]]\n"
          ]
        }
      ]
    }
  ]
}