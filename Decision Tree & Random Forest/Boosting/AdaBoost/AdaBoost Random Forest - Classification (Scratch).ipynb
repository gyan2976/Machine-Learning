{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AdaBoost Random Forest - Classification (Scratch)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcBMz4mDPvTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "txc-dnUTvyY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index, threshold, left, right):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right"
      ],
      "metadata": {
        "id": "fKxwY8KLz8Lk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafNode:\n",
        "    def __init__(self, y):\n",
        "        self.labels, self.counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    def predicted_class(self):\n",
        "        return self.labels[np.argmax(self.counts)]"
      ],
      "metadata": {
        "id": "YuBsOYfTz8OV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"gini\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y, sample_weights=None):\n",
        "        self.tree = self._build_tree(X, y, sample_weights=sample_weights)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0, sample_weights=None):\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        if len(unique_classes) == 1 or depth >= self.max_depth or num_samples < self.min_samples_split:\n",
        "            return LeafNode(y)\n",
        "\n",
        "        best_split = self._best_split(X, y, num_features, sample_weights)\n",
        "        if best_split is None:\n",
        "            return LeafNode(y)\n",
        "\n",
        "        left_indices = best_split['indices_left']\n",
        "        right_indices = best_split['indices_right']\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1, sample_weights[left_indices])\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1, sample_weights[right_indices])\n",
        "\n",
        "        return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, num_features, sample_weights=None):\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                indices_left = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                indices_right = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(indices_left) > 0 and len(indices_right) > 0:\n",
        "                    gain = self._information_gain(y, indices_left, indices_right, sample_weights)\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'indices_left': indices_left,\n",
        "                            'indices_right': indices_right\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _information_gain(self, y, left_indices, right_indices, sample_weights=None):\n",
        "        impurity_before = self._impurity(y, sample_weights)\n",
        "        impurity_left = self._impurity(y[left_indices], sample_weights[left_indices])\n",
        "        impurity_right = self._impurity(y[right_indices], sample_weights[right_indices])\n",
        "\n",
        "        weighted_impurity = (len(left_indices) / len(y)) * impurity_left + (len(right_indices) / len(y)) * impurity_right\n",
        "        return impurity_before - weighted_impurity\n",
        "\n",
        "    def _impurity(self, y, sample_weights=None):\n",
        "        if self.criterion == \"gini\":\n",
        "            return self._gini_impurity(y, sample_weights)\n",
        "        elif self.criterion == \"entropy\":\n",
        "            return self._entropy_impurity(y, sample_weights)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
        "\n",
        "    def _gini_impurity(self, y, sample_weights=None):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        if sample_weights is not None:\n",
        "            counts = np.bincount(y, weights=sample_weights)\n",
        "        probabilities = counts / np.sum(counts)\n",
        "        return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "\n",
        "    def _entropy_impurity(self, y, sample_weights=None):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        if sample_weights is not None:\n",
        "            counts = np.bincount(y, weights=sample_weights)\n",
        "        probabilities = counts / np.sum(counts)\n",
        "        return -np.sum(probabilities * np.log2(probabilities + 1e-15))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, LeafNode):\n",
        "            return tree.predicted_class()\n",
        "        else:\n",
        "            if x[tree.feature_index] <= tree.threshold:\n",
        "                return self._traverse_tree(x, tree.left)\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree.right)"
      ],
      "metadata": {
        "id": "q4ASKWh-z8Q-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoostForest:\n",
        "    def __init__(self, n_estimators=5, max_depth=3, min_samples_split=2, criterion=\"gini\"):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.trees = []\n",
        "        self.tree_weights = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        sample_weights = np.full(n_samples, (1 / n_samples))\n",
        "        self.trees = []\n",
        "        self.tree_weights = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion)\n",
        "            tree.fit(X, y, sample_weights=sample_weights)\n",
        "\n",
        "            predictions = tree.predict(X)\n",
        "            error = np.sum(sample_weights * (predictions != y)) / np.sum(sample_weights)\n",
        "\n",
        "            tree_weight = 0.5 * np.log((1 - error) / (error + 1e-10))  # Avoid division by zero\n",
        "            self.trees.append(tree)\n",
        "            self.tree_weights.append(tree_weight)\n",
        "\n",
        "            # Update sample weights\n",
        "            sample_weights *= np.exp(-tree_weight * y * (2 * (predictions == y) - 1))\n",
        "            sample_weights /= np.sum(sample_weights)\n",
        "\n",
        "    def predict(self, X):\n",
        "        weighted_predictions = np.zeros(X.shape[0])\n",
        "        for tree, tree_weight in zip(self.trees, self.tree_weights):\n",
        "            predictions = tree.predict(X)\n",
        "            weighted_predictions += tree_weight * predictions\n",
        "\n",
        "        return np.sign(weighted_predictions).astype(int)"
      ],
      "metadata": {
        "id": "U25gzznCXa7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "6s0308H30K4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y[y == 0] = -1  # Adjust labels to {-1, 1} for AdaBoost binary classification"
      ],
      "metadata": {
        "id": "_DLnivNXyWys"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "gL9ODNSH0IaT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace -1 with a valid class label (e.g., 0)\n",
        "y_train_replaced = np.where(y_train == -1, 0, y_train)"
      ],
      "metadata": {
        "id": "zdDurKHV0Idk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AdaBoostForest on the updated data\n",
        "ada_boost_forest = AdaBoostForest(n_estimators=5, max_depth=3)\n",
        "ada_boost_forest.fit(X_train, y_train_replaced)"
      ],
      "metadata": {
        "id": "tXZaXc350Ifz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions = ada_boost_forest.predict(X_test)\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XUQ1Izk0IiX",
        "outputId": "1846616c-b511-4f27-cbd2-ff97dd3cb192"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"AdaBoost Forest Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt4rQvYl0IlH",
        "outputId": "5fbf7797-2f88-4464-96c5-f4248e9e7aa3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Forest Accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ige0wr000In_",
        "outputId": "419efbd7-f315-4dda-b004-bac3a1c1d2e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[ 0 10  0  0]\n",
            " [ 0  0  0  0]\n",
            " [ 0  0  9  0]\n",
            " [ 0  0 11  0]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00        10\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.45      1.00      0.62         9\n",
            "           2       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.30        30\n",
            "   macro avg       0.11      0.25      0.16        30\n",
            "weighted avg       0.13      0.30      0.19        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}