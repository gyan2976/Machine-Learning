{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**XGBoost Random Forest - Classification (Scratch)**"
      ],
      "metadata": {
        "id": "4iyAzJiK0vH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "uG1RJOrJoxDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_diabetes, load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier"
      ],
      "metadata": {
        "id": "WBhBkbuTkfZV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "nOofeYvYAa3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index, threshold, left, right):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right"
      ],
      "metadata": {
        "id": "tU0DaNY7adin"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafNode:\n",
        "    def __init__(self, y):\n",
        "        self.labels, self.counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    def predicted_class(self):\n",
        "        return self.labels[np.argmax(self.counts)]"
      ],
      "metadata": {
        "id": "rElHfqAUcFWb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"gini\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        if len(unique_classes) == 1 or depth >= self.max_depth or num_samples < self.min_samples_split:\n",
        "            return LeafNode(y)\n",
        "\n",
        "        best_split = self._best_split(X, y, num_features)\n",
        "        if best_split is None:\n",
        "            return LeafNode(y)\n",
        "\n",
        "        left_indices = best_split['indices_left']\n",
        "        right_indices = best_split['indices_right']\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, num_features):\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                indices_left = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                indices_right = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(indices_left) > 0 and len(indices_right) > 0:\n",
        "                    gain = self._information_gain(y, indices_left, indices_right)\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'indices_left': indices_left,\n",
        "                            'indices_right': indices_right\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _information_gain(self, y, left_indices, right_indices):\n",
        "        impurity_before = self._impurity(y)\n",
        "        impurity_left = self._impurity(y[left_indices])\n",
        "        impurity_right = self._impurity(y[right_indices])\n",
        "\n",
        "        weighted_impurity = (len(left_indices) / len(y)) * impurity_left + (len(right_indices) / len(y)) * impurity_right\n",
        "        return impurity_before - weighted_impurity\n",
        "\n",
        "    def _impurity(self, y):\n",
        "        if self.criterion == \"gini\":\n",
        "            return self._gini_impurity(y)\n",
        "        elif self.criterion == \"entropy\":\n",
        "            return self._entropy_impurity(y)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "    def _entropy_impurity(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        return -np.sum(probabilities * np.log2(probabilities + 1e-15))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, LeafNode):\n",
        "            return tree.predicted_class()\n",
        "        else:\n",
        "            if x[tree.feature_index] <= tree.threshold:\n",
        "                return self._traverse_tree(x, tree.left)\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree.right)\n",
        "\n",
        "    def print_tree(self, tree=None, depth=0):\n",
        "        if tree is None:\n",
        "            tree = self.tree\n",
        "        if isinstance(tree, LeafNode):\n",
        "            print(f\"{'   ' * depth}Predict {tree.predicted_class()}\")\n",
        "        else:\n",
        "            print(f\"{'   ' * depth}Feature {tree.feature_index} <= {tree.threshold}\")\n",
        "            self.print_tree(tree.left, depth + 1)\n",
        "            print(f\"{'   ' * depth}Feature {tree.feature_index} > {tree.threshold}\")\n",
        "            self.print_tree(tree.right, depth + 1)"
      ],
      "metadata": {
        "id": "Qw7qqd8vcmvk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForestWithXGBoost:\n",
        "    def __init__(self, n_forests=1, n_trees=5, max_depth=10, min_samples_split=2, criterion=\"gini\", xgboost_trees=2):\n",
        "        self.n_forests = n_forests  # Number of random forests\n",
        "        self.n_trees = n_trees  # Number of trees in each random forest\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.xgboost_trees = xgboost_trees  # Number of XGBoost trees\n",
        "        self.forests = []  # Store the forests\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for forest_index in range(self.n_forests):\n",
        "            print(f\"\\nTraining Random Forest {forest_index + 1} with {self.n_trees} decision trees.\")\n",
        "            trees = []\n",
        "            for tree_index in range(self.n_trees):\n",
        "                tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion)\n",
        "                sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
        "                X_sample = X[sample_indices]\n",
        "                y_sample = y[sample_indices]\n",
        "                tree.fit(X_sample, y_sample)\n",
        "                trees.append(tree)\n",
        "\n",
        "                # Print the tree structure\n",
        "                print(f\"\\nTree {tree_index + 1} Structure:\")\n",
        "                tree.print_tree()\n",
        "\n",
        "            self.forests.append(trees)\n",
        "\n",
        "            # Train XGBoost model\n",
        "            dtrain = xgb.DMatrix(X, label=y)\n",
        "            params = {\n",
        "                'max_depth': self.max_depth,\n",
        "                'objective': 'multi:softmax',\n",
        "                'num_class': len(np.unique(y)),\n",
        "                'eval_metric': 'mlogloss'\n",
        "            }\n",
        "            self.xgb_model = xgb.train(params, dtrain, num_boost_round=self.xgboost_trees)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for forest_index, trees in enumerate(self.forests):\n",
        "            forest_predictions = []\n",
        "            print(f\"\\nForest {forest_index + 1} predictions:\")\n",
        "            for tree_index, tree in enumerate(trees):\n",
        "                tree_pred = tree.predict(X)\n",
        "                forest_predictions.append(tree_pred)\n",
        "                print(f\"  Tree {tree_index + 1} predictions: {tree_pred}\")\n",
        "\n",
        "            predictions.append(forest_predictions)\n",
        "\n",
        "            # Print combined predictions for the forest\n",
        "            combined_forest_preds = np.array([Counter(row).most_common(1)[0][0] for row in np.vstack(forest_predictions).T])\n",
        "            print(f\"Combined predictions for Forest {forest_index + 1}: {combined_forest_preds}\")\n",
        "\n",
        "        # XGBoost predictions\n",
        "        dtest = xgb.DMatrix(X)\n",
        "        xgb_preds = self.xgb_model.predict(dtest)\n",
        "\n",
        "        # Print XGBoost predictions\n",
        "        print(f\"XGBoost predictions: {xgb_preds}\")\n",
        "\n",
        "        # Return combined forest predictions and XGBoost predictions\n",
        "        return combined_forest_preds, xgb_preds\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        # Get combined predictions from the first forest for evaluation\n",
        "        combined_forest_preds, xgb_preds = self.predict(X)\n",
        "\n",
        "        # Calculate the classification report and confusion matrix\n",
        "        print(\"\\nClassification Report for Random Forest:\")\n",
        "        print(classification_report(y, combined_forest_preds))\n",
        "        print(\"Confusion Matrix for Random Forest:\")\n",
        "        print(confusion_matrix(y, combined_forest_preds))\n",
        "\n",
        "        print(\"\\nClassification Report for XGBoost:\")\n",
        "        print(classification_report(y, xgb_preds))\n",
        "        print(\"Confusion Matrix for XGBoost:\")\n",
        "        print(confusion_matrix(y, xgb_preds))"
      ],
      "metadata": {
        "id": "U51p5hzOcmyu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForestEnsemble:\n",
        "    def __init__(self, n_forests=3, n_trees=5, max_depth=10, min_samples_split=2, criterion=\"gini\", xgboost_trees=2):\n",
        "        self.n_forests = n_forests\n",
        "        self.forests = [\n",
        "            RandomForestWithXGBoost(n_trees=n_trees, max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, xgboost_trees=xgboost_trees)\n",
        "            for _ in range(n_forests)\n",
        "        ]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i, forest in enumerate(self.forests):\n",
        "            print(f\"Training Random Forest {i+1} with {forest.n_trees} trees...\")\n",
        "            forest.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        all_forest_predictions = np.array([forest.predict(X) for forest in self.forests])\n",
        "        final_predictions = [Counter(row).most_common(1)[0][0] for row in all_forest_predictions.T]\n",
        "        return final_predictions\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return accuracy\n",
        "\n",
        "    def print_forest_details(self):\n",
        "        for i, forest in enumerate(self.forests):\n",
        "            print(f\"\\nRandom Forest {i+1}:\")\n",
        "            forest.print_trees()"
      ],
      "metadata": {
        "id": "us9IlKWpcm2X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "Lu6Y70-5dno-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "Rp48l2cKdl9e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ll9XaQZtdmAV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ensemble of random forests\n",
        "ensemble = RandomForestEnsemble(n_forests=3, n_trees=5, max_depth=10, min_samples_split=2, criterion=\"gini\", xgboost_trees=2)"
      ],
      "metadata": {
        "id": "Bx1Hwaj9dmF-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RandomForest with XGBoost integration\n",
        "n_forests = 3  # Number of random forests\n",
        "forest = RandomForestWithXGBoost(n_forests=n_forests, n_trees=5, max_depth=10, min_samples_split=2, criterion=\"gini\", xgboost_trees=2)\n",
        "forest.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CldvsDmdmHR",
        "outputId": "60d0939c-3d71-4076-8e84-c3f2719c4158"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Random Forest 1 with 5 decision trees.\n",
            "\n",
            "Tree 1 Structure:\n",
            "Feature 2 <= 1.7\n",
            "   Predict 0\n",
            "Feature 2 > 1.7\n",
            "   Feature 2 <= 4.9\n",
            "      Feature 3 <= 1.5\n",
            "         Predict 1\n",
            "      Feature 3 > 1.5\n",
            "         Feature 1 <= 2.8\n",
            "            Predict 2\n",
            "         Feature 1 > 2.8\n",
            "            Predict 1\n",
            "   Feature 2 > 4.9\n",
            "      Feature 2 <= 5.0\n",
            "         Feature 0 <= 6.3\n",
            "            Predict 2\n",
            "         Feature 0 > 6.3\n",
            "            Predict 1\n",
            "      Feature 2 > 5.0\n",
            "         Predict 2\n",
            "\n",
            "Tree 2 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.7\n",
            "      Feature 2 <= 5.0\n",
            "         Feature 0 <= 4.9\n",
            "            Feature 1 <= 2.4\n",
            "               Predict 1\n",
            "            Feature 1 > 2.4\n",
            "               Predict 2\n",
            "         Feature 0 > 4.9\n",
            "            Predict 1\n",
            "      Feature 2 > 5.0\n",
            "         Predict 2\n",
            "   Feature 3 > 1.7\n",
            "      Predict 2\n",
            "\n",
            "Tree 3 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.7\n",
            "      Feature 2 <= 4.9\n",
            "         Feature 3 <= 1.6\n",
            "            Predict 1\n",
            "         Feature 3 > 1.6\n",
            "            Predict 2\n",
            "      Feature 2 > 4.9\n",
            "         Feature 1 <= 2.2\n",
            "            Predict 2\n",
            "         Feature 1 > 2.2\n",
            "            Feature 0 <= 6.7\n",
            "               Predict 1\n",
            "            Feature 0 > 6.7\n",
            "               Predict 2\n",
            "   Feature 3 > 1.7\n",
            "      Feature 2 <= 4.8\n",
            "         Feature 0 <= 5.9\n",
            "            Predict 1\n",
            "         Feature 0 > 5.9\n",
            "            Predict 2\n",
            "      Feature 2 > 4.8\n",
            "         Predict 2\n",
            "\n",
            "Tree 4 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.8\n",
            "      Feature 2 <= 4.7\n",
            "         Predict 1\n",
            "      Feature 2 > 4.7\n",
            "         Feature 0 <= 5.9\n",
            "            Predict 1\n",
            "         Feature 0 > 5.9\n",
            "            Predict 2\n",
            "   Feature 2 > 4.8\n",
            "      Feature 3 <= 1.5\n",
            "         Feature 0 <= 6.3\n",
            "            Predict 2\n",
            "         Feature 0 > 6.3\n",
            "            Predict 1\n",
            "      Feature 3 > 1.5\n",
            "         Predict 2\n",
            "\n",
            "Tree 5 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.5\n",
            "      Predict 1\n",
            "   Feature 3 > 1.5\n",
            "      Feature 0 <= 5.9\n",
            "         Feature 1 <= 3.0\n",
            "            Predict 2\n",
            "         Feature 1 > 3.0\n",
            "            Predict 1\n",
            "      Feature 0 > 5.9\n",
            "         Predict 2\n",
            "\n",
            "Training Random Forest 2 with 5 decision trees.\n",
            "\n",
            "Tree 1 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.8\n",
            "      Predict 1\n",
            "   Feature 2 > 4.8\n",
            "      Feature 1 <= 2.7\n",
            "         Feature 3 <= 1.6\n",
            "            Feature 0 <= 6.1\n",
            "               Feature 1 <= 2.6\n",
            "                  Predict 2\n",
            "               Feature 1 > 2.6\n",
            "                  Predict 1\n",
            "            Feature 0 > 6.1\n",
            "               Predict 1\n",
            "         Feature 3 > 1.6\n",
            "            Predict 2\n",
            "      Feature 1 > 2.7\n",
            "         Predict 2\n",
            "\n",
            "Tree 2 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.7\n",
            "      Predict 1\n",
            "   Feature 2 > 4.7\n",
            "      Feature 3 <= 1.7\n",
            "         Feature 0 <= 6.3\n",
            "            Predict 2\n",
            "         Feature 0 > 6.3\n",
            "            Feature 0 <= 6.9\n",
            "               Predict 1\n",
            "            Feature 0 > 6.9\n",
            "               Predict 2\n",
            "      Feature 3 > 1.7\n",
            "         Predict 2\n",
            "\n",
            "Tree 3 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.7\n",
            "      Feature 0 <= 4.9\n",
            "         Predict 2\n",
            "      Feature 0 > 4.9\n",
            "         Predict 1\n",
            "   Feature 2 > 4.7\n",
            "      Feature 2 <= 4.8\n",
            "         Feature 0 <= 5.9\n",
            "            Predict 1\n",
            "         Feature 0 > 5.9\n",
            "            Predict 2\n",
            "      Feature 2 > 4.8\n",
            "         Feature 3 <= 1.6\n",
            "            Feature 0 <= 6.0\n",
            "               Feature 1 <= 2.2\n",
            "                  Predict 2\n",
            "               Feature 1 > 2.2\n",
            "                  Predict 1\n",
            "            Feature 0 > 6.0\n",
            "               Predict 2\n",
            "         Feature 3 > 1.6\n",
            "            Predict 2\n",
            "\n",
            "Tree 4 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.7\n",
            "      Feature 2 <= 5.1\n",
            "         Feature 1 <= 2.2\n",
            "            Feature 0 <= 5.0\n",
            "               Predict 1\n",
            "            Feature 0 > 5.0\n",
            "               Predict 2\n",
            "         Feature 1 > 2.2\n",
            "            Predict 1\n",
            "      Feature 2 > 5.1\n",
            "         Predict 2\n",
            "   Feature 3 > 1.7\n",
            "      Feature 2 <= 4.8\n",
            "         Feature 0 <= 5.9\n",
            "            Predict 1\n",
            "         Feature 0 > 5.9\n",
            "            Predict 2\n",
            "      Feature 2 > 4.8\n",
            "         Predict 2\n",
            "\n",
            "Tree 5 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.6\n",
            "      Feature 2 <= 4.9\n",
            "         Predict 1\n",
            "      Feature 2 > 4.9\n",
            "         Feature 0 <= 6.0\n",
            "            Predict 1\n",
            "         Feature 0 > 6.0\n",
            "            Predict 2\n",
            "   Feature 3 > 1.6\n",
            "      Predict 2\n",
            "\n",
            "Training Random Forest 3 with 5 decision trees.\n",
            "\n",
            "Tree 1 Structure:\n",
            "Feature 2 <= 1.7\n",
            "   Predict 0\n",
            "Feature 2 > 1.7\n",
            "   Feature 2 <= 4.7\n",
            "      Predict 1\n",
            "   Feature 2 > 4.7\n",
            "      Feature 2 <= 4.9\n",
            "         Feature 0 <= 6.2\n",
            "            Feature 1 <= 3.0\n",
            "               Predict 2\n",
            "            Feature 1 > 3.0\n",
            "               Predict 1\n",
            "         Feature 0 > 6.2\n",
            "            Predict 1\n",
            "      Feature 2 > 4.9\n",
            "         Predict 2\n",
            "\n",
            "Tree 2 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.7\n",
            "      Feature 3 <= 1.6\n",
            "         Predict 1\n",
            "      Feature 3 > 1.6\n",
            "         Predict 2\n",
            "   Feature 2 > 4.7\n",
            "      Feature 2 <= 5.0\n",
            "         Feature 0 <= 6.0\n",
            "            Predict 2\n",
            "         Feature 0 > 6.0\n",
            "            Feature 3 <= 1.7\n",
            "               Predict 1\n",
            "            Feature 3 > 1.7\n",
            "               Predict 2\n",
            "      Feature 2 > 5.0\n",
            "         Predict 2\n",
            "\n",
            "Tree 3 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.7\n",
            "      Feature 1 <= 2.2\n",
            "         Feature 0 <= 5.0\n",
            "            Predict 1\n",
            "         Feature 0 > 5.0\n",
            "            Predict 2\n",
            "      Feature 1 > 2.2\n",
            "         Predict 1\n",
            "   Feature 3 > 1.7\n",
            "      Predict 2\n",
            "\n",
            "Tree 4 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 2 <= 4.7\n",
            "      Predict 1\n",
            "   Feature 2 > 4.7\n",
            "      Feature 3 <= 1.7\n",
            "         Feature 0 <= 6.1\n",
            "            Predict 2\n",
            "         Feature 0 > 6.1\n",
            "            Feature 0 <= 6.7\n",
            "               Predict 1\n",
            "            Feature 0 > 6.7\n",
            "               Predict 2\n",
            "      Feature 3 > 1.7\n",
            "         Predict 2\n",
            "\n",
            "Tree 5 Structure:\n",
            "Feature 2 <= 1.9\n",
            "   Predict 0\n",
            "Feature 2 > 1.9\n",
            "   Feature 3 <= 1.5\n",
            "      Feature 2 <= 4.9\n",
            "         Predict 1\n",
            "      Feature 2 > 4.9\n",
            "         Predict 2\n",
            "   Feature 3 > 1.5\n",
            "      Feature 2 <= 4.8\n",
            "         Feature 1 <= 3.0\n",
            "            Predict 2\n",
            "         Feature 1 > 3.0\n",
            "            Predict 1\n",
            "      Feature 2 > 4.8\n",
            "         Predict 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions = forest.predict(X_test)\n",
        "print(\"Predicted classes from each random forest:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8ql6EPWdmJu",
        "outputId": "a053ce7f-df93-460b-8489-528267c1c11d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forest 1 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 1 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 1: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Forest 2 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 2: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Forest 3 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 3: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "XGBoost predictions: [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
            " 2. 2. 2. 2. 0. 0.]\n",
            "Predicted classes from each random forest: (array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
            "       0, 2, 2, 2, 2, 2, 0, 0]), array([1., 0., 2., 1., 1., 0., 1., 2., 1., 1., 2., 0., 0., 0., 0., 1., 2.,\n",
            "       1., 1., 2., 0., 2., 0., 2., 2., 2., 2., 2., 0., 0.], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "forest.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e16FWuohL4b",
        "outputId": "fdced7cf-d956-4208-8dd3-9012dd000eec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forest 1 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 1 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 1: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Forest 2 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 2: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Forest 3 predictions:\n",
            "  Tree 1 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 2 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 3 predictions: [1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 4 predictions: [1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "  Tree 5 predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Combined predictions for Forest 3: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "XGBoost predictions: [1. 0. 2. 1. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 1. 2. 1. 1. 2. 0. 2. 0. 2.\n",
            " 2. 2. 2. 2. 0. 0.]\n",
            "\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report for XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix for XGBoost:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    }
  ]
}